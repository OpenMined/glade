{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cdf02b-db6d-4d51-a359-c5fdf5851777",
   "metadata": {},
   "source": [
    "# Figure out if there are any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7532d6fa-475e-487e-be74-bc5829b0f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clinvar format\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n",
    "# 1\t66926\t3385321\tAG\tA\t.\t.\tALLELEID=3544463;CLNDISDB=Human_Phenotype_Ontology:HP:0000547,MONDO:MONDO:0019200,MeSH:D012174,MedGen:C0035334,OMIM:268000,OMIM:PS268000,Orphanet:791;CLNDN=Retinitis_pigmentosa;CLNHGVS=NC_000001.10:g.66927del;CLNREVSTAT=criteria_provided,_single_submitter;CLNSIG=Uncertain_significance;CLNSIGSCV=SCV005419006;CLNVC=Deletion;CLNVCSO=SO:0000159;GENEINFO=OR4F5:79501;MC=SO:0001627|intron_variant;ORIGIN=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30579495-8891-4d41-85c4-88b6c20a4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23andme format\n",
    "# rsid,chromosome,position,genotype\n",
    "# rs12564807,1,734462,AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd05d8d8-4a14-4980-9914-ccb92558e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7d9401-e33a-4f00-a7bc-654d194b5494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping iXXXXX IDs before duplicate checks: 60,880\n",
      "Duplicate RSIDs (different positions): 0 -> work/duplicate_rsids.csv\n",
      "Duplicate positions (multiple RSIDs): 35 -> work/duplicate_positions.csv\n",
      "✅ Wrote 1,056,706 rows to work/merged_rsids_positions.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import re\n",
    "\n",
    "DATA_DIR = Path(\"./downloads/family-genome-dataset\")\n",
    "WORK_DIR = Path(\"./work\")\n",
    "OUTPUT = WORK_DIR / \"merged_rsids_positions.csv\"\n",
    "DUP_RSID_FILE = WORK_DIR / \"duplicate_rsids.csv\"\n",
    "DUP_POS_FILE = WORK_DIR / \"duplicate_positions.csv\"\n",
    "\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def tuple_str(values):\n",
    "    \"\"\"Format a sequence like ('a','b') as (a,b,) with no quotes, trailing comma.\"\"\"\n",
    "    vals = [str(v) for v in values]\n",
    "    return \"(\" + \",\".join(vals) + (\",\" if len(vals) >= 1 else \"\") + \")\"\n",
    "\n",
    "csv_paths = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "all_dfs = []\n",
    "\n",
    "for path in csv_paths:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "            data_lines = [ln for ln in f if not ln.lstrip().startswith(\"#\") and ln.strip()]\n",
    "        if not data_lines:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            io.StringIO(\"\".join(data_lines)),\n",
    "            header=None,\n",
    "            names=[\"rsid\", \"chromosome\", \"position\", \"genotype\"],\n",
    "            dtype={\"rsid\": \"string\", \"chromosome\": \"string\", \"genotype\": \"string\"},\n",
    "            low_memory=False,\n",
    "        )\n",
    "        df = df[[\"rsid\", \"chromosome\", \"position\"]].copy()\n",
    "        df[\"position\"] = pd.to_numeric(df[\"position\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df = df.dropna(subset=[\"rsid\", \"position\"])\n",
    "\n",
    "        all_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {path.name}: {e}\", file=sys.stderr)\n",
    "\n",
    "if not all_dfs:\n",
    "    pd.DataFrame(columns=[\"rsid\", \"chromosome\", \"position\"]).to_csv(OUTPUT, index=False)\n",
    "    print(f\"⚠️ No records parsed. Wrote empty header to {OUTPUT}\", file=sys.stderr)\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# Merge and drop exact duplicates (rsid, chr, pos) silently\n",
    "merged = pd.concat(all_dfs, ignore_index=True)\n",
    "distinct = merged.drop_duplicates(subset=[\"rsid\", \"chromosome\", \"position\"]).reset_index(drop=True)\n",
    "\n",
    "# ---------- Drop all iXXXXX ids FIRST ----------\n",
    "is_i_id = distinct[\"rsid\"].str.match(r\"^i\\d+$\", na=False)\n",
    "i_count = int(is_i_id.sum())\n",
    "print(f\"Dropping iXXXXX IDs before duplicate checks: {i_count:,}\")\n",
    "distinct = distinct.loc[~is_i_id].copy()\n",
    "\n",
    "# ---------- Duplicate RSIDs with different positions ----------\n",
    "g = distinct.groupby(\"rsid\", dropna=False)\n",
    "dup_rsid_groups = g.filter(lambda d: d[[\"chromosome\",\"position\"]].drop_duplicates().shape[0] > 1)\n",
    "\n",
    "if dup_rsid_groups.empty:\n",
    "    dup_rsid_df = pd.DataFrame(columns=[\"rsid\", \"chromosomes\", \"positions\"])\n",
    "else:\n",
    "    rows = []\n",
    "    for rsid, sub in dup_rsid_groups.groupby(\"rsid\"):\n",
    "        pairs = sub[[\"chromosome\",\"position\"]].drop_duplicates().sort_values(by=[\"chromosome\",\"position\"])\n",
    "        chroms = [str(c) for c in pairs[\"chromosome\"].tolist()]\n",
    "        poss = [str(int(p)) for p in pairs[\"position\"].tolist()]\n",
    "        rows.append({\n",
    "            \"rsid\": rsid,\n",
    "            \"chromosomes\": tuple_str(chroms),\n",
    "            \"positions\": tuple_str(poss),\n",
    "        })\n",
    "    dup_rsid_df = pd.DataFrame(rows, columns=[\"rsid\",\"chromosomes\",\"positions\"])\n",
    "\n",
    "dup_rsid_df.to_csv(DUP_RSID_FILE, index=False)\n",
    "print(f\"Duplicate RSIDs (different positions): {len(dup_rsid_df):,} -> {DUP_RSID_FILE}\")\n",
    "\n",
    "# ---------- Duplicate positions with different RSIDs ----------\n",
    "gp = distinct.groupby([\"chromosome\",\"position\"], dropna=False)\n",
    "dup_pos_groups = gp.filter(lambda d: d[\"rsid\"].nunique() > 1)\n",
    "\n",
    "if dup_pos_groups.empty:\n",
    "    dup_pos_df = pd.DataFrame(columns=[\"rsid\", \"chromosomes\", \"positions\"])\n",
    "else:\n",
    "    rows = []\n",
    "    for (chr_, pos_), sub in dup_pos_groups.groupby([\"chromosome\",\"position\"]):\n",
    "        rsids = sub[\"rsid\"].drop_duplicates().sort_values(\n",
    "            key=lambda s: s.str.extract(r\"(\\d+)\", expand=False).fillna(\"0\").astype(int)\n",
    "        ).tolist()\n",
    "        chroms = sub[\"chromosome\"].drop_duplicates().tolist()\n",
    "        rows.append({\n",
    "            \"rsid\": tuple_str(rsids),\n",
    "            \"chromosomes\": tuple_str([str(c) for c in chroms]),\n",
    "            \"positions\": str(int(pos_)),\n",
    "        })\n",
    "    dup_pos_df = pd.DataFrame(rows, columns=[\"rsid\",\"chromosomes\",\"positions\"])\\\n",
    "                   .sort_values(by=[\"positions\",\"rsid\"]).reset_index(drop=True)\n",
    "\n",
    "dup_pos_df.to_csv(DUP_POS_FILE, index=False)\n",
    "print(f\"Duplicate positions (multiple RSIDs): {len(dup_pos_df):,} -> {DUP_POS_FILE}\")\n",
    "\n",
    "final_df = distinct.copy()\n",
    "final_df = final_df.sort_values(\n",
    "    by=[\"chromosome\", \"position\", \"rsid\"],\n",
    "    key=lambda col: (\n",
    "        pd.to_numeric(col, errors=\"coerce\")\n",
    "        if col.name in [\"chromosome\", \"position\"]\n",
    "        else col\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Save final\n",
    "final_df.to_csv(OUTPUT, index=False)\n",
    "print(f\"✅ Wrote {len(final_df):,} rows to {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e990a9-5c26-46c6-92bf-e89a623bbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it looks like there are a few positions with multiple rsids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f70810-c21f-4019-8961-a1c3b0084a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsid,chromosomes,positions\n",
      "\"(rs62642906,rs62642946,)\",\"(12,)\",103310863\n",
      "\"(rs1131454,rs3741981,)\",\"(12,)\",113348870\n",
      "\"(rs770990,rs113777878,)\",\"(12,)\",133525460\n",
      "\"(rs8176719,rs56231711,)\",\"(9,)\",136132909\n",
      "\"(rs2822142,rs114956511,)\",\"(21,)\",15181318\n",
      "\"(rs61748415,rs61748416,)\",\"(X,)\",153296798\n",
      "\"(rs61748408,rs61748409,)\",\"(X,)\",153296811\n",
      "\"(rs9341274,rs373506129,)\",\"(Y,)\",15591446\n",
      "\"(rs57077886,rs58727209,)\",\"(1,)\",156084738\n"
     ]
    }
   ],
   "source": [
    "!head work/duplicate_positions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f507cb6e-7463-4b29-81bf-17698eb36be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ncbi.nlm.nih.gov/snp/?term=rs62642906 #indel\n",
    "# https://www.ncbi.nlm.nih.gov/snp/?term=rs62642946 #snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9fce6c-3676-4712-b854-b7a86ea5cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the first one to see whats up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1ab140-81f4-4a2b-adae-7bfadab271b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def grep_csvs(search: str, base_dir: str = \"downloads/**/*.csv\"):\n",
    "    \"\"\"Search recursively through CSV files for a given string.\"\"\"\n",
    "    matches = []\n",
    "    for path in glob.glob(base_dir, recursive=True):\n",
    "        with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"ignore\") as f:\n",
    "            for ln in f:\n",
    "                if search in ln:\n",
    "                    matches.append((path, ln.strip()))\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8ea9f6-8474-4939-9c14-8d48b823117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloads/family-genome-dataset/Mother Genome.csv → rs62642906,12,103310863,II\n",
      "downloads/family-genome-dataset/Mother Genome.csv → rs62642946,12,103310863,AA\n",
      "downloads/family-genome-dataset/Child 1 Genome.csv → rs62642906,12,103310863,II\n",
      "downloads/family-genome-dataset/Child 1 Genome.csv → rs62642946,12,103310863,AA\n",
      "downloads/family-genome-dataset/Father Genome.csv → rs62642906,12,103310863,II\n",
      "downloads/family-genome-dataset/Father Genome.csv → rs62642946,12,103310863,AA\n"
     ]
    }
   ],
   "source": [
    "for path, line in grep_csvs(\"103310863\"):\n",
    "    print(path, \"→\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e72cfad7-d363-4c23-8c5c-94421d9a1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay looks like some people have both which makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b13657-2f73-4eed-a2b4-1941c6b7561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check another common one like APO-E pos 45411941"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc6ec512-73df-467f-b496-9b26da2c88e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloads/family-genome-dataset/Mother Genome.csv → rs429358,19,45411941,TT\n",
      "downloads/family-genome-dataset/Child 2 Genome.csv → rs429358,19,45411941,TT\n",
      "downloads/family-genome-dataset/Child 1 Genome.csv → rs429358,19,45411941,TT\n",
      "downloads/family-genome-dataset/Child 3 Genome.csv → rs429358,19,45411941,TT\n",
      "downloads/family-genome-dataset/Father Genome.csv → rs429358,19,45411941,TT\n"
     ]
    }
   ],
   "source": [
    "for path, line in grep_csvs(\"45411941\"):\n",
    "    print(path, \"→\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5196fc5-2452-4d4f-8003-432f439faa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ncbi.nlm.nih.gov/snp/?term=rs429358\n",
    "# \"\"\"\n",
    "# Alleles:T>C [Show Flanks]Chromosome:19:44908684 (GRCh38)\n",
    "# 19:45411941 (GRCh37)\n",
    "# \"\"\"\n",
    "# looks valid to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "344d8537-386f-48dc-8a77-60db97adcf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay now lets compare each file to see if there are common subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8bfea99-24f4-4809-bf56-ed654cc0d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RSID set identity groups (files sharing identical unique RSIDs) ===\n",
      "Group 1: 3 file(s) -> Child 1 Genome.csv, Father Genome.csv, Mother Genome.csv  [|RSIDs|=553773]\n",
      "Group 2: 2 file(s) -> Child 2 Genome.csv, Child 3 Genome.csv  [|RSIDs|=614871]\n",
      "\n",
      "ℹ️ Found 2 distinct RSID sets across 5 files.\n",
      "\n",
      "=== Pairwise RSID set comparison (counts) ===\n",
      "Child 1 Genome.csv vs Child 2 Genome.csv: |∩|=111938, |A−B|=441835, |B−A|=502933\n",
      "Child 1 Genome.csv vs Child 3 Genome.csv: |∩|=111938, |A−B|=441835, |B−A|=502933\n",
      "Child 1 Genome.csv vs Father Genome.csv: |∩|=553773, |A−B|=0, |B−A|=0\n",
      "Child 1 Genome.csv vs Mother Genome.csv: |∩|=553773, |A−B|=0, |B−A|=0\n",
      "Child 2 Genome.csv vs Child 3 Genome.csv: |∩|=614871, |A−B|=0, |B−A|=0\n",
      "Child 2 Genome.csv vs Father Genome.csv: |∩|=111938, |A−B|=502933, |B−A|=441835\n",
      "Child 2 Genome.csv vs Mother Genome.csv: |∩|=111938, |A−B|=502933, |B−A|=441835\n",
      "Child 3 Genome.csv vs Father Genome.csv: |∩|=111938, |A−B|=502933, |B−A|=441835\n",
      "Child 3 Genome.csv vs Mother Genome.csv: |∩|=111938, |A−B|=502933, |B−A|=441835\n",
      "Father Genome.csv vs Mother Genome.csv: |∩|=553773, |A−B|=0, |B−A|=0\n",
      "\n",
      "✅ Wrote common RSID subset with chr/pos (n=111938) to work/rsids_common_subset.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "DATA_DIR = Path(\"./downloads/family-genome-dataset\")\n",
    "MERGED = Path(\"./work/merged_rsids_positions.csv\")  # optional canonical positions\n",
    "COMMON_OUT = Path(\"./work/rsids_common_subset.csv\")\n",
    "\n",
    "# Optional: load merged file (sanity check/use later if needed)\n",
    "if MERGED.exists():\n",
    "    merged_df = pd.read_csv(\n",
    "        MERGED,\n",
    "        dtype={\"rsid\": \"string\", \"chromosome\": \"string\"},\n",
    "        low_memory=False,\n",
    "    )\n",
    "    merged_df[\"rsid\"] = merged_df[\"rsid\"].astype(\"string\")\n",
    "    # Make quick lookup: rsid -> (chromosome, position)\n",
    "    merged_lookup = {\n",
    "        r[\"rsid\"]: (str(r[\"chromosome\"]), int(r[\"position\"]))\n",
    "        for _, r in merged_df.dropna(subset=[\"rsid\", \"chromosome\", \"position\"]).iterrows()\n",
    "    }\n",
    "else:\n",
    "    merged_df = pd.DataFrame(columns=[\"rsid\", \"chromosome\", \"position\"])\n",
    "    merged_lookup = {}\n",
    "\n",
    "# ---- load per-file unique RSIDs (rs* only) and keep chr/pos ----\n",
    "csv_paths = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "if not csv_paths:\n",
    "    print(f\"⚠️ No CSVs found in {DATA_DIR}\", file=sys.stderr)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "file_rsids: dict[str, set[str]] = {}\n",
    "file_maps: dict[str, dict[str, tuple[str, int]]] = {}  # filename -> {rsid: (chrom, pos)}\n",
    "\n",
    "rs_re = re.compile(r\"^rs\\d+$\")\n",
    "\n",
    "for path in csv_paths:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "            data_lines = [ln for ln in f if not ln.lstrip().startswith(\"#\") and ln.strip()]\n",
    "        if not data_lines:\n",
    "            print(f\"⚠️ {path.name} has no data rows after skipping comments.\", file=sys.stderr)\n",
    "            file_rsids[path.name] = set()\n",
    "            file_maps[path.name] = {}\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            io.StringIO(\"\".join(data_lines)),\n",
    "            header=None,\n",
    "            names=[\"rsid\", \"chromosome\", \"position\", \"genotype\"],\n",
    "            usecols=[0, 1, 2],\n",
    "            dtype={\"rsid\": \"string\", \"chromosome\": \"string\"},\n",
    "            low_memory=False,\n",
    "        )\n",
    "\n",
    "        # Clean + filter to rsIDs\n",
    "        df = df.dropna(subset=[\"rsid\", \"chromosome\", \"position\"]).copy()\n",
    "        df = df[df[\"rsid\"].str.match(rs_re, na=False)]\n",
    "\n",
    "        # Coerce position to int where possible\n",
    "        # (23andMe files are integers; be robust if strings sneak in)\n",
    "        df[\"position\"] = pd.to_numeric(df[\"position\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df = df.dropna(subset=[\"position\"]).copy()\n",
    "        df[\"position\"] = df[\"position\"].astype(int)\n",
    "\n",
    "        # Build set and map\n",
    "        file_rsids[path.name] = set(df[\"rsid\"].tolist())\n",
    "        file_maps[path.name] = {\n",
    "            r[\"rsid\"]: (str(r[\"chromosome\"]), int(r[\"position\"]))\n",
    "            for _, r in df.iterrows()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {path.name}: {e}\", file=sys.stderr)\n",
    "        file_rsids[path.name] = set()\n",
    "        file_maps[path.name] = {}\n",
    "\n",
    "if not file_rsids:\n",
    "    print(\"⚠️ No RSIDs collected.\", file=sys.stderr)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ---- group files by identical RSID set ----\n",
    "groups: dict[frozenset[str], list[str]] = defaultdict(list)\n",
    "for fname, rset in file_rsids.items():\n",
    "    groups[frozenset(rset)].append(fname)\n",
    "\n",
    "all_identical = (len(groups) == 1)\n",
    "\n",
    "print(\"\\n=== RSID set identity groups (files sharing identical unique RSIDs) ===\")\n",
    "for i, (rset, files) in enumerate(groups.items(), start=1):\n",
    "    print(f\"Group {i}: {len(files)} file(s) -> {', '.join(sorted(files))}  [|RSIDs|={len(rset)}]\")\n",
    "\n",
    "if all_identical:\n",
    "    print(\"\\n✅ All files share an identical RSID set.\")\n",
    "else:\n",
    "    print(f\"\\nℹ️ Found {len(groups)} distinct RSID sets across {len(file_rsids)} files.\")\n",
    "\n",
    "# ---- pairwise difference summary (counts only) ----\n",
    "print(\"\\n=== Pairwise RSID set comparison (counts) ===\")\n",
    "fnames = sorted(file_rsids.keys())\n",
    "for i, fa in enumerate(fnames):\n",
    "    for fb in fnames[i+1:]:\n",
    "        A, B = file_rsids[fa], file_rsids[fb]\n",
    "        inter = len(A & B)\n",
    "        only_a = len(A - B)\n",
    "        only_b = len(B - A)\n",
    "        print(f\"{fa} vs {fb}: |∩|={inter}, |A−B|={only_a}, |B−A|={only_b}\")\n",
    "\n",
    "# ---- compute RSIDs common to ALL files ----\n",
    "common_rsids = None\n",
    "for s in file_rsids.values():\n",
    "    common_rsids = s if common_rsids is None else (common_rsids & s)\n",
    "common_rsids = common_rsids or set()\n",
    "\n",
    "# ---- choose chromosome/position per common rsid ----\n",
    "# Preference: MERGED canonical -> majority vote across files -> first seen\n",
    "rows = []\n",
    "for rsid in sorted(common_rsids):\n",
    "    if rsid in merged_lookup:\n",
    "        chrom, pos = merged_lookup[rsid]\n",
    "    else:\n",
    "        # Collect all (chrom,pos) across files that have this rsid\n",
    "        pairs = []\n",
    "        for fmap in file_maps.values():\n",
    "            if rsid in fmap:\n",
    "                pairs.append(fmap[rsid])\n",
    "        if pairs:\n",
    "            chrom, pos = Counter(pairs).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Shouldn't happen if rsid is in the intersection, but be safe\n",
    "            chrom, pos = (\"NA\", -1)\n",
    "    rows.append((rsid, str(chrom), int(pos)))\n",
    "\n",
    "common_df = pd.DataFrame(rows, columns=[\"rsid\", \"chromosome\", \"position\"])\n",
    "common_df.to_csv(COMMON_OUT, index=False)\n",
    "print(f\"\\n✅ Wrote common RSID subset with chr/pos (n={len(common_df)}) to {COMMON_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d183a8e0-7adc-4e45-b33c-250a42f25c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rs62642906,12,103310863\n",
      "rs62642946,12,103310863\n"
     ]
    }
   ],
   "source": [
    "# make sure all positions are accounted for even the duplicates\n",
    "!cat work/merged_rsids_positions.csv | grep 103310863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fc211a1-4044-4088-b631-467ca7af1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently they changed the snps a lot over different generations so we need to get the superset from clinvar\n",
    "# This seems to agree\n",
    "# https://www.reddit.com/r/23andme/comments/3dd3lp/snp_coverage_analysiscomparisons_23andme_v3v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6b4e51b-9e0e-4770-ba2e-0039612f832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def guess_23andme_version(csv_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Guess 23andMe chip version (v2, v3, v4, v5) based on row count.\n",
    "    Skips header/comment lines starting with '#'.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "        for ln in f:\n",
    "            if not ln.lstrip().startswith(\"#\") and ln.strip():\n",
    "                count += 1\n",
    "\n",
    "    # Heuristic thresholds\n",
    "    if 500_000 <= count < 600_000:\n",
    "        version = \"v2 (~550k SNPs)\"\n",
    "    elif 900_000 <= count < 1_000_000:\n",
    "        version = \"v3 (~960k SNPs)\"\n",
    "    elif 580_000 <= count < 620_000:\n",
    "        version = \"v4 (~600k SNPs)\"\n",
    "    elif 620_000 <= count < 660_000:\n",
    "        version = \"v5 (~640k SNPs)\"\n",
    "    else:\n",
    "        version = \"Unknown/ambiguous\"\n",
    "\n",
    "    return f\"{csv_path.name}: {count:,} rows → {version}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2ac9c87-6910-4894-b837-ae874f4e3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child 1 Genome.csv: 601,802 rows → v4 (~600k SNPs)\n",
      "Child 2 Genome.csv: 631,983 rows → v5 (~640k SNPs)\n",
      "Child 3 Genome.csv: 631,983 rows → v5 (~640k SNPs)\n",
      "Father Genome.csv: 601,802 rows → v4 (~600k SNPs)\n",
      "Mother Genome.csv: 601,802 rows → v4 (~600k SNPs)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"./downloads/family-genome-dataset\")\n",
    "\n",
    "csv_paths = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "for path in csv_paths:\n",
    "    print(guess_23andme_version(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a59542-df80-4468-98b9-b19a7f37795c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
